{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c1b6035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\genimg_bovagau\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c47e90e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load model gốc và Embedding bạn vừa train xong\n",
    "base_model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "embedding_path_bo = r\"C:\\Users\\MY LAPTOP\\Downloads\\bovagau-textual-inversion\\learned_embeds-steps-3000-Bo2.safetensors\"\n",
    "# embedding_path_gau = r\"C:\\Users\\MY LAPTOP\\Downloads\\bovagau-textual-inversion\\learned_embeds-steps-3000-Gau.safetensors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ae717d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 196/196 [00:00<00:00, 406.44it/s, Materializing param=text_model.final_layer_norm.weight]\n",
      "CLIPTextModel LOAD REPORT from: C:\\Users\\MY LAPTOP\\.cache\\huggingface\\hub\\models--runwayml--stable-diffusion-v1-5\\snapshots\\451f4fe16113bff5a5d2269ed5ad43b0592e9a14\\text_encoder\n",
      "Key                                | Status     |  | \n",
      "-----------------------------------+------------+--+-\n",
      "text_model.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Loading weights: 100%|██████████| 396/396 [00:00<00:00, 396.57it/s, Materializing param=visual_projection.weight]\n",
      "StableDiffusionSafetyChecker LOAD REPORT from: C:\\Users\\MY LAPTOP\\.cache\\huggingface\\hub\\models--runwayml--stable-diffusion-v1-5\\snapshots\\451f4fe16113bff5a5d2269ed5ad43b0592e9a14\\safety_checker\n",
      "Key                                               | Status     |  | \n",
      "--------------------------------------------------+------------+--+-\n",
      "vision_model.vision_model.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:02<00:00,  3.22it/s]\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    base_model_id\n",
    ")\n",
    "pipeline.load_textual_inversion(embedding_path_bo)\n",
    "# pipeline.load_textual_inversion(embedding_path_gau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "adf8c181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\"<bo>\" in pipeline.tokenizer.get_vocab())\n",
    "print(\"<Gau>\" in pipeline.tokenizer.get_vocab())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f0092427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\"Trump\" in pipeline.tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f36d45a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:49<02:54,  4.48s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m image = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTrump wearing a suit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mblurry, low quality\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# guidance_scale=10.0,\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# num_inference_steps=30,\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.images[\u001b[32m0\u001b[39m]\n\u001b[32m      8\u001b[39m image.save(\u001b[33m\"\u001b[39m\u001b[33moutput1_0_trump.png\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\genimg_bovagau\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\genimg_bovagau\\Lib\\site-packages\\diffusers\\pipelines\\stable_diffusion\\pipeline_stable_diffusion.py:1041\u001b[39m, in \u001b[36mStableDiffusionPipeline.__call__\u001b[39m\u001b[34m(self, prompt, height, width, num_inference_steps, timesteps, sigmas, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, guidance_rescale, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[39m\n\u001b[32m   1038\u001b[39m     latent_model_input = \u001b[38;5;28mself\u001b[39m.scheduler.scale_model_input(latent_model_input, t)\n\u001b[32m   1040\u001b[39m \u001b[38;5;66;03m# predict the noise residual\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1041\u001b[39m noise_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlatent_model_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimestep_cond\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimestep_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1049\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m   1051\u001b[39m \u001b[38;5;66;03m# perform guidance\u001b[39;00m\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.do_classifier_free_guidance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\genimg_bovagau\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\genimg_bovagau\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\genimg_bovagau\\Lib\\site-packages\\diffusers\\models\\unets\\unet_2d_condition.py:1153\u001b[39m, in \u001b[36mUNet2DConditionModel.forward\u001b[39m\u001b[34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[39m\n\u001b[32m   1150\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_adapter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(down_intrablock_additional_residuals) > \u001b[32m0\u001b[39m:\n\u001b[32m   1151\u001b[39m         additional_residuals[\u001b[33m\"\u001b[39m\u001b[33madditional_residuals\u001b[39m\u001b[33m\"\u001b[39m] = down_intrablock_additional_residuals.pop(\u001b[32m0\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1153\u001b[39m     sample, res_samples = \u001b[43mdownsample_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m=\u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_residuals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1163\u001b[39m     sample, res_samples = downsample_block(hidden_states=sample, temb=emb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\genimg_bovagau\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\genimg_bovagau\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\genimg_bovagau\\Lib\\site-packages\\diffusers\\models\\unets\\unet_2d_blocks.py:1269\u001b[39m, in \u001b[36mCrossAttnDownBlock2D.forward\u001b[39m\u001b[34m(self, hidden_states, temb, encoder_hidden_states, attention_mask, cross_attention_kwargs, encoder_attention_mask, additional_residuals)\u001b[39m\n\u001b[32m   1260\u001b[39m     hidden_states = attn(\n\u001b[32m   1261\u001b[39m         hidden_states,\n\u001b[32m   1262\u001b[39m         encoder_hidden_states=encoder_hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1266\u001b[39m         return_dict=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1267\u001b[39m     )[\u001b[32m0\u001b[39m]\n\u001b[32m   1268\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1269\u001b[39m     hidden_states = \u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1270\u001b[39m     hidden_states = attn(\n\u001b[32m   1271\u001b[39m         hidden_states,\n\u001b[32m   1272\u001b[39m         encoder_hidden_states=encoder_hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1276\u001b[39m         return_dict=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1277\u001b[39m     )[\u001b[32m0\u001b[39m]\n\u001b[32m   1279\u001b[39m \u001b[38;5;66;03m# apply additional residuals to the output of the last pair of resnet and attention blocks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\genimg_bovagau\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\genimg_bovagau\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\genimg_bovagau\\Lib\\site-packages\\diffusers\\models\\resnet.py:341\u001b[39m, in \u001b[36mResnetBlock2D.forward\u001b[39m\u001b[34m(self, input_tensor, temb, *args, **kwargs)\u001b[39m\n\u001b[32m    338\u001b[39m     input_tensor = \u001b[38;5;28mself\u001b[39m.downsample(input_tensor)\n\u001b[32m    339\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.downsample(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.time_emb_proj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    344\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.skip_time_act:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\genimg_bovagau\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\genimg_bovagau\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\genimg_bovagau\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\genimg_bovagau\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "image = pipeline(\n",
    "    prompt=\"Trump wearing a suit\",\n",
    "    negative_prompt=\"blurry, low quality\",\n",
    "    # guidance_scale=10.0,\n",
    "    # num_inference_steps=30,\n",
    "    \n",
    "    ).images[0]\n",
    "image.save(\"output1_0_trump.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d3b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [03:13<00:00,  3.87s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "image = pipeline(\n",
    "    prompt=\"a cartoon drawing of <Gau> wearing a suit\",\n",
    "    negative_prompt=\"blurry, low quality, realistic\",\n",
    "    # guidance_scale=10.0,\n",
    "    # num_inference_steps=30,\n",
    "    \n",
    "    ).images[0]\n",
    "image.save(\"output1_0_normal.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c358376e",
   "metadata": {},
   "source": [
    "## LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddf043b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_merged_path = r\"C:\\Users\\MY LAPTOP\\OneDrive\\Documents\\GitHub\\Meme-Generator\\models\\SD-1-5-bo-gau\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9e99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 196/196 [00:00<00:00, 296.31it/s, Materializing param=text_model.final_layer_norm.weight]\n",
      "CLIPTextModel LOAD REPORT from: C:\\Users\\MY LAPTOP\\.cache\\huggingface\\hub\\models--runwayml--stable-diffusion-v1-5\\snapshots\\451f4fe16113bff5a5d2269ed5ad43b0592e9a14\\text_encoder\n",
      "Key                                | Status     |  | \n",
      "-----------------------------------+------------+--+-\n",
      "text_model.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Loading weights: 100%|██████████| 396/396 [00:01<00:00, 304.35it/s, Materializing param=visual_projection.weight]\n",
      "StableDiffusionSafetyChecker LOAD REPORT from: C:\\Users\\MY LAPTOP\\.cache\\huggingface\\hub\\models--runwayml--stable-diffusion-v1-5\\snapshots\\451f4fe16113bff5a5d2269ed5ad43b0592e9a14\\safety_checker\n",
      "Key                                               | Status     |  | \n",
      "--------------------------------------------------+------------+--+-\n",
      "vision_model.vision_model.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:05<00:00,  1.24it/s]\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã xong! Hãy dùng đường dẫn 'C:\\Users\\MY LAPTOP\\OneDrive\\Documents\\GitHub\\Meme-Generator\\models\\SD-1-5-bo-gau' để train LoRA.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16)\n",
    "pipe.load_textual_inversion(embedding_path_bo) # Load <Bo> vào\n",
    "# pipe.load_textual_inversion(embedding_path_gau) \n",
    "\n",
    "\n",
    "# 2. Lưu lại thành một model hoàn toàn mới (đã chứa Bo)\n",
    "# Model này sẽ hiểu <Bo> là gì mà không cần load file .bin nữa\n",
    "\n",
    "pipe.save_pretrained(output_merged_path, safe_serialization=True)\n",
    "\n",
    "print(f\"Đã xong! Hãy dùng đường dẫn '{output_merged_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea121be6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc01d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 196/196 [00:00<00:00, 359.79it/s, Materializing param=text_model.final_layer_norm.weight]\n",
      "Loading weights: 100%|██████████| 396/396 [00:01<00:00, 385.88it/s, Materializing param=visual_projection.weight]\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:02<00:00,  3.02it/s]\n"
     ]
    }
   ],
   "source": [
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    output_merged_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load LoRA weights into UNet using PEFT\n",
    "pipeline.unet = PeftModel.from_pretrained(\n",
    "    pipeline.unet,\n",
    "    r\"C:\\Users\\MY LAPTOP\\Downloads\\Bo_final\",\n",
    "    is_trainable=False\n",
    ")\n",
    "\n",
    "pipeline = pipeline.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3ccb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:09<00:00,  5.45it/s]\n"
     ]
    }
   ],
   "source": [
    "image = pipeline(\n",
    "    prompt=\"a cartoon drawing of a <bo> wearing a suit\",\n",
    "    negative_prompt=\"blurry, low quality, realistic\",\n",
    "    # guidance_scale=10.0,\n",
    "    # num_inference_steps=30,\n",
    "    \n",
    "    ).images[0]\n",
    "image.save(\"outputLora.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18240cc",
   "metadata": {},
   "source": [
    "# add Controlnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b25d95f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\genimg_bovagau\\Lib\\site-packages\\dwpose\\util2.py:17: UserWarning: USE_SYMLINKS not set successfully. Using default value: False to download models.\n",
      "  warnings.warn(\"USE_SYMLINKS not set successfully. Using default value: False to download models.\")\n"
     ]
    }
   ],
   "source": [
    "from diffusers import (\n",
    "    StableDiffusionControlNetPipeline, \n",
    "    ControlNetModel, \n",
    ")\n",
    "from dwpose import DwposeDetector\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "controlnet_model_id = \"lllyasviel/control_v11p_sd15_openpose\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "551f7587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ControlNet Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\genimg_bovagau\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:202: UserWarning: The `local_dir_use_symlinks` argument is deprecated and ignored in `hf_hub_download`. Downloading to a local directory does not use symlinks anymore.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading ControlNet Model...\")\n",
    "controlnet = ControlNetModel.from_pretrained(controlnet_model_id, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed34e4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 396/396 [00:01<00:00, 368.06it/s, Materializing param=visual_projection.weight]\n",
      "Loading weights: 100%|██████████| 196/196 [00:00<00:00, 375.44it/s, Materializing param=text_model.final_layer_norm.weight]\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:02<00:00,  3.21it/s]\n"
     ]
    }
   ],
   "source": [
    "pipeline = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    output_merged_path,\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load LoRA weights into UNet using PEFT\n",
    "pipeline.unet = PeftModel.from_pretrained(\n",
    "    pipeline.unet,\n",
    "    r\"C:\\Users\\MY LAPTOP\\Downloads\\Bo_final\",\n",
    "    is_trainable=False\n",
    ")\n",
    "\n",
    "pipeline = pipeline.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "380fea60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DWPose estimator...\n",
      "\n",
      "DWPose: Using yolox_l.onnx for bbox detection and dw-ll_ucoco_384.onnx for pose estimation\n",
      "DWPose: Caching OpenCV DNN module yolox_l.onnx on cv2.DNN...\n",
      "DWPose: Caching OpenCV DNN module dw-ll_ucoco_384.onnx on cv2.DNN...\n",
      "DWPose: Bbox 928.16ms\n",
      "DWPose: Pose 210.81ms on 1 people\n",
      "\n",
      "Pose extracted.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Extract Pose (ControlNet Input) ---\n",
    "meme_image  = load_image(\"Picture1.png\").convert(\"RGB\")    # the meme/content reference\n",
    "\n",
    "print(\"Loading DWPose estimator...\")\n",
    "pose_estimator = DwposeDetector.from_pretrained_default()\n",
    "pose_skeleton = pose_estimator(\n",
    "    meme_image, \n",
    "    output_type=\"pil\",  # This is the key setting\n",
    "    include_body=True,\n",
    "    include_hands=True, \n",
    "    include_face=True\n",
    ")\n",
    "print(\"Pose extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb6499bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:19<00:00,  2.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- 9. Tinh chỉnh ---\n",
    "prompt = \"a cartoon drawing of a <bo> wearing a suit\"\n",
    "negative_prompt = \"blurry, low quality, realistic, text, NSFW\"\n",
    "\n",
    "\n",
    "# --- 10. Generate Image ---\n",
    "# generator = torch.Generator(device=\"cuda\").manual_seed(91)\n",
    "\n",
    "output_image = pipeline(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    image=pose_skeleton,            # ControlNet input\n",
    "    # num_inference_steps=30,\n",
    "    # guidance_scale=6.0,\n",
    "    # generator=generator\n",
    ").images[0]\n",
    "output_image.save(\"outputLoraControlNet.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genimg_bovagau",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
