{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4391014b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc7f74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f3d6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import warnings\n",
    "from contextlib import nullcontext\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "import safetensors\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "\n",
    "from packaging import version\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "import diffusers\n",
    "from diffusers.models.autoencoders.autoencoder_kl import AutoencoderKL\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "from diffusers.pipelines.pipeline_utils import DiffusionPipeline\n",
    "from diffusers.schedulers.scheduling_dpmsolver_multistep import DPMSolverMultistepScheduler\n",
    "from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion import StableDiffusionPipeline\n",
    "from diffusers import UNet2DConditionModel\n",
    "\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.utils import check_min_version, is_wandb_available\n",
    "from diffusers.utils.hub_utils import load_or_create_model_card, populate_model_card\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "\n",
    "if is_wandb_available():\n",
    "    import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9371567",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key = \"d9d14819dddd8a35a353b5c0b087e0f60d717140\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61a5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "if version.parse(version.parse(PIL.__version__).base_version) >= version.parse(\"9.1.0\"):\n",
    "    PIL_INTERPOLATION = {\n",
    "        \"linear\": PIL.Image.Resampling.BILINEAR,\n",
    "        \"bilinear\": PIL.Image.Resampling.BILINEAR,\n",
    "        \"bicubic\": PIL.Image.Resampling.BICUBIC,\n",
    "        \"lanczos\": PIL.Image.Resampling.LANCZOS,\n",
    "        \"nearest\": PIL.Image.Resampling.NEAREST,\n",
    "    }\n",
    "else:\n",
    "    PIL_INTERPOLATION = {\n",
    "        \"linear\": PIL.Image.LINEAR,\n",
    "        \"bilinear\": PIL.Image.BILINEAR,\n",
    "        \"bicubic\": PIL.Image.BICUBIC,\n",
    "        \"lanczos\": PIL.Image.LANCZOS,\n",
    "        \"nearest\": PIL.Image.NEAREST,\n",
    "    }\n",
    "\n",
    "check_min_version(\"0.36.0.dev0\")\n",
    "\n",
    "logger = get_logger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ba77fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration for textual inversion training\"\"\"\n",
    "    # Model settings\n",
    "    pretrained_model_name_or_path: str = \"runwayml/stable-diffusion-v1-5\"\n",
    "    revision: Optional[str] = None\n",
    "    variant: Optional[str] = None\n",
    "    tokenizer_name: Optional[str] = None\n",
    "    \n",
    "    # Data settings\n",
    "    train_data_dir: str = \"./data/train\"\n",
    "    placeholder_token: str = \"<my-token>\"\n",
    "    initializer_token: str = \"a\"  # This will be used to initialize the embedding\n",
    "    learnable_property: str = \"object\"  # \"object\" or \"style\"\n",
    "    repeats: int = 100\n",
    "    resolution: int = 512\n",
    "    center_crop: bool = False\n",
    "    \n",
    "    # Training settings\n",
    "    train_batch_size: int = 4\n",
    "    num_train_epochs: int = 100\n",
    "    max_train_steps: int = 5000\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    gradient_checkpointing: bool = False\n",
    "    learning_rate: float = 5e-4\n",
    "    scale_lr: bool = False\n",
    "    lr_scheduler: str = \"constant\"\n",
    "    lr_warmup_steps: int = 500\n",
    "    lr_num_cycles: int = 1\n",
    "    dataloader_num_workers: int = 0\n",
    "    \n",
    "    # Optimizer settings\n",
    "    adam_beta1: float = 0.9\n",
    "    adam_beta2: float = 0.999\n",
    "    adam_weight_decay: float = 1e-2\n",
    "    adam_epsilon: float = 1e-08\n",
    "    \n",
    "    # Saving settings\n",
    "    output_dir: str = \"text-inversion-model\"\n",
    "    save_steps: int = 500\n",
    "    save_as_full_pipeline: bool = False\n",
    "    checkpointing_steps: int = 500\n",
    "    checkpoints_total_limit: Optional[int] = None\n",
    "    resume_from_checkpoint: Optional[str] = None\n",
    "    no_safe_serialization: bool = False\n",
    "    \n",
    "    # Validation settings\n",
    "    validation_prompt: Optional[str] = None\n",
    "    num_validation_images: int = 4\n",
    "    validation_steps: int = 100\n",
    "    \n",
    "    # Logging settings\n",
    "    logging_dir: str = \"logs\"\n",
    "    report_to: str = \"wandb\"\n",
    "    wandb_project_name: str = \"textual-inversion\"\n",
    "    \n",
    "    # Other settings\n",
    "    seed: Optional[int] = 42\n",
    "    mixed_precision: str = \"no\"  # \"no\", \"fp16\", \"bf16\"\n",
    "    allow_tf32: bool = False\n",
    "    enable_xformers_memory_efficient_attention: bool = False\n",
    "    num_vectors: int = 1\n",
    "    \n",
    "    # Hub settings\n",
    "    push_to_hub: bool = False\n",
    "    hub_token: Optional[str] = None\n",
    "    hub_model_id: Optional[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e0cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextualInversionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root,\n",
    "        tokenizer,\n",
    "        size=512,\n",
    "        repeats=100,\n",
    "        interpolation=\"bicubic\",\n",
    "        flip_p=0.5,\n",
    "        set=\"train\",\n",
    "        placeholder_token=\"*\",\n",
    "        learnable_property=\"object\",\n",
    "        center_crop=False,\n",
    "    ):\n",
    "        self.data_root = data_root\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size = size\n",
    "        self.placeholder_token = placeholder_token\n",
    "        self.learnable_property = learnable_property\n",
    "        self.center_crop = center_crop\n",
    "        self.flip_p = flip_p\n",
    "\n",
    "        # Load images and captions\n",
    "        self.image_paths = []\n",
    "        self.captions = []\n",
    "        \n",
    "        for file_path in os.listdir(self.data_root):\n",
    "            if file_path.lower().endswith(('.png', '.jpg', '.jpeg', '.webp')):\n",
    "                img_path = os.path.join(self.data_root, file_path)\n",
    "                self.image_paths.append(img_path)\n",
    "                \n",
    "                # Look for caption file (same name with .txt extension)\n",
    "                caption_path = os.path.splitext(img_path)[0] + '.txt'\n",
    "                if os.path.exists(caption_path):\n",
    "                    with open(caption_path, 'r') as f:\n",
    "                        caption = f.read().strip()\n",
    "                else:\n",
    "                    caption = \"\"  # Default empty caption\n",
    "                self.captions.append(caption)\n",
    "\n",
    "        self.num_images = len(self.image_paths)\n",
    "        self._length = self.num_images\n",
    "\n",
    "        if set == \"train\":\n",
    "            self._length = self.num_images * repeats\n",
    "\n",
    "        self.interpolation = {\n",
    "            \"linear\": PIL_INTERPOLATION[\"linear\"],\n",
    "            \"bilinear\": PIL_INTERPOLATION[\"bilinear\"],\n",
    "            \"bicubic\": PIL_INTERPOLATION[\"bicubic\"],\n",
    "            \"lanczos\": PIL_INTERPOLATION[\"lanczos\"],\n",
    "        }[interpolation]\n",
    "\n",
    "        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        example = {}\n",
    "        idx = i % self.num_images\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "\n",
    "        if not image.mode == \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "\n",
    "        # Use template with caption\n",
    "        caption = self.captions[idx]\n",
    "        if self.learnable_property == \"object\":\n",
    "            if caption:\n",
    "                text = f\"A photo of {self.placeholder_token}, {caption}\"\n",
    "            else:\n",
    "                text = f\"A photo of {self.placeholder_token}\"\n",
    "        else:\n",
    "            if caption:\n",
    "                text = f\"A photo in the style of {self.placeholder_token}, {caption}\"\n",
    "            else:\n",
    "                text = f\"A photo in the style of {self.placeholder_token}\"\n",
    "\n",
    "        example[\"input_ids\"] = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids[0]\n",
    "\n",
    "        # default to score-sde preprocessing\n",
    "        img = np.array(image).astype(np.uint8)\n",
    "\n",
    "        if self.center_crop:\n",
    "            crop = min(img.shape[0], img.shape[1])\n",
    "            h, w = img.shape[0], img.shape[1]\n",
    "            img = img[(h - crop) // 2 : (h + crop) // 2, (w - crop) // 2 : (w + crop) // 2]\n",
    "\n",
    "        image = Image.fromarray(img)\n",
    "        image = image.resize((self.size, self.size), resample=self.interpolation)\n",
    "\n",
    "        image = self.flip_transform(image)\n",
    "        image = np.array(image).astype(np.uint8)\n",
    "        image = (image / 127.5 - 1.0).astype(np.float32)\n",
    "\n",
    "        example[\"pixel_values\"] = torch.from_numpy(image).permute(2, 0, 1)\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd0b957",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_model_card(repo_id: str, images: list = None, base_model: str = None, repo_folder: str = None):\n",
    "    img_str = \"\"\n",
    "    if images is not None:\n",
    "        for i, image in enumerate(images):\n",
    "            image.save(os.path.join(repo_folder, f\"image_{i}.png\"))\n",
    "            img_str += f\"![img_{i}](./image_{i}.png)\\n\"\n",
    "    model_description = f\"\"\"\n",
    "# Textual inversion text2image fine-tuning - {repo_id}\n",
    "These are textual inversion adaption weights for {base_model}. You can find some example images in the following. \\n\n",
    "{img_str}\n",
    "\"\"\"\n",
    "    model_card = load_or_create_model_card(\n",
    "        repo_id_or_path=repo_id,\n",
    "        from_training=True,\n",
    "        license=\"creativeml-openrail-m\",\n",
    "        base_model=base_model,\n",
    "        model_description=model_description,\n",
    "        inference=True,\n",
    "    )\n",
    "\n",
    "    tags = [\n",
    "        \"stable-diffusion\",\n",
    "        \"stable-diffusion-diffusers\",\n",
    "        \"text-to-image\",\n",
    "        \"diffusers\",\n",
    "        \"textual_inversion\",\n",
    "        \"diffusers-training\",\n",
    "    ]\n",
    "    model_card = populate_model_card(model_card, tags=tags)\n",
    "\n",
    "    model_card.save(os.path.join(repo_folder, \"README.md\"))\n",
    "\n",
    "\n",
    "def log_validation(text_encoder, tokenizer, unet, vae, args, accelerator, weight_dtype, epoch):\n",
    "    logger.info(\n",
    "        f\"Running validation... \\n Generating {args.num_validation_images} images with prompt:\"\n",
    "        f\" {args.validation_prompt}.\"\n",
    "    )\n",
    "    # create pipeline (note: unet and vae are loaded again in float32)\n",
    "    pipeline = DiffusionPipeline.from_pretrained(\n",
    "        args.pretrained_model_name_or_path,\n",
    "        text_encoder=accelerator.unwrap_model(text_encoder),\n",
    "        tokenizer=tokenizer,\n",
    "        unet=unet,\n",
    "        vae=vae,\n",
    "        safety_checker=None,\n",
    "        revision=args.revision,\n",
    "        variant=args.variant,\n",
    "        torch_dtype=weight_dtype,\n",
    "    )\n",
    "    pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
    "    pipeline = pipeline.to(accelerator.device)\n",
    "    pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "    # run inference\n",
    "    generator = None if args.seed is None else torch.Generator(device=accelerator.device).manual_seed(args.seed)\n",
    "    images = []\n",
    "    for _ in range(args.num_validation_images):\n",
    "        if torch.backends.mps.is_available():\n",
    "            autocast_ctx = nullcontext()\n",
    "        else:\n",
    "            autocast_ctx = torch.autocast(accelerator.device.type)\n",
    "\n",
    "        with autocast_ctx:\n",
    "            image = pipeline(args.validation_prompt, num_inference_steps=25, generator=generator).images[0]\n",
    "        images.append(image)\n",
    "\n",
    "    for tracker in accelerator.trackers:\n",
    "        if tracker.name == \"tensorboard\":\n",
    "            np_images = np.stack([np.asarray(img) for img in images])\n",
    "            tracker.writer.add_images(\"validation\", np_images, epoch, dataformats=\"NHWC\")\n",
    "        if tracker.name == \"wandb\":\n",
    "            tracker.log(\n",
    "                {\n",
    "                    \"validation\": [\n",
    "                        wandb.Image(image, caption=f\"{i}: {args.validation_prompt}\") for i, image in enumerate(images)\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "\n",
    "    del pipeline\n",
    "    torch.cuda.empty_cache()\n",
    "    return images\n",
    "\n",
    "\n",
    "def save_progress(text_encoder, placeholder_token_ids, accelerator, args, save_path, safe_serialization=True):\n",
    "    logger.info(\"Saving embeddings\")\n",
    "    learned_embeds = (\n",
    "        accelerator.unwrap_model(text_encoder)\n",
    "        .get_input_embeddings()\n",
    "        .weight[min(placeholder_token_ids) : max(placeholder_token_ids) + 1]\n",
    "    )\n",
    "    learned_embeds_dict = {args.placeholder_token: learned_embeds.detach().cpu()}\n",
    "\n",
    "    if safe_serialization:\n",
    "        safetensors.torch.save_file(learned_embeds_dict, save_path, metadata={\"format\": \"pt\"})\n",
    "    else:\n",
    "        torch.save(learned_embeds_dict, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696b8345",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextualInversionTrainer:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.accelerator = None\n",
    "        self.tokenizer = None\n",
    "        self.text_encoder = None\n",
    "        self.vae = None\n",
    "        self.unet = None\n",
    "        self.noise_scheduler = None\n",
    "        self.optimizer = None\n",
    "        self.lr_scheduler = None\n",
    "        self.train_dataloader = None\n",
    "        self.placeholder_token_ids = None\n",
    "        \n",
    "    def setup(self):\n",
    "        \"\"\"Initialize all components\"\"\"\n",
    "        args = self.config\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "            datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "            level=logging.INFO,\n",
    "        )\n",
    "        \n",
    "        # Setup accelerator\n",
    "        logging_dir = os.path.join(args.output_dir, args.logging_dir)\n",
    "        accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)\n",
    "        self.accelerator = Accelerator(\n",
    "            gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "            mixed_precision=args.mixed_precision,\n",
    "            log_with=args.report_to,\n",
    "            project_config=accelerator_project_config,\n",
    "        )\n",
    "\n",
    "        # Disable AMP for MPS.\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.accelerator.native_amp = False\n",
    "\n",
    "        logger.info(self.accelerator.state, main_process_only=False)\n",
    "        if self.accelerator.is_local_main_process:\n",
    "            transformers.utils.logging.set_verbosity_warning()\n",
    "            diffusers.utils.logging.set_verbosity_info()\n",
    "        else:\n",
    "            transformers.utils.logging.set_verbosity_error()\n",
    "            diffusers.utils.logging.set_verbosity_error()\n",
    "\n",
    "        # Set seed\n",
    "        if args.seed is not None:\n",
    "            set_seed(args.seed)\n",
    "\n",
    "        # Create output directory\n",
    "        if self.accelerator.is_main_process:\n",
    "            if args.output_dir is not None:\n",
    "                os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "            if args.push_to_hub:\n",
    "                self.repo_id = create_repo(\n",
    "                    repo_id=args.hub_model_id or Path(args.output_dir).name, \n",
    "                    exist_ok=True, \n",
    "                    token=args.hub_token\n",
    "                ).repo_id\n",
    "\n",
    "        # Load tokenizer\n",
    "        if args.tokenizer_name:\n",
    "            self.tokenizer = CLIPTokenizer.from_pretrained(args.tokenizer_name)\n",
    "        elif args.pretrained_model_name_or_path:\n",
    "            self.tokenizer = CLIPTokenizer.from_pretrained(\n",
    "                args.pretrained_model_name_or_path, subfolder=\"tokenizer\"\n",
    "            )\n",
    "\n",
    "        # Load models\n",
    "        self.noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "            args.pretrained_model_name_or_path, subfolder=\"scheduler\"\n",
    "        )\n",
    "        self.text_encoder = CLIPTextModel.from_pretrained(\n",
    "            args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision\n",
    "        )\n",
    "        self.vae = AutoencoderKL.from_pretrained(\n",
    "            args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision, variant=args.variant\n",
    "        )\n",
    "        self.unet = UNet2DConditionModel.from_pretrained(\n",
    "            args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.revision, variant=args.variant\n",
    "        )\n",
    "\n",
    "        # Add placeholder tokens\n",
    "        placeholder_tokens = [args.placeholder_token]\n",
    "        if args.num_vectors < 1:\n",
    "            raise ValueError(f\"--num_vectors has to be larger or equal to 1, but is {args.num_vectors}\")\n",
    "\n",
    "        additional_tokens = []\n",
    "        for i in range(1, args.num_vectors):\n",
    "            additional_tokens.append(f\"{args.placeholder_token}_{i}\")\n",
    "        placeholder_tokens += additional_tokens\n",
    "\n",
    "        num_added_tokens = self.tokenizer.add_tokens(placeholder_tokens)\n",
    "        if num_added_tokens != args.num_vectors:\n",
    "            raise ValueError(\n",
    "                f\"The tokenizer already contains the token {args.placeholder_token}. Please pass a different\"\n",
    "                \" `placeholder_token` that is not already in the tokenizer.\"\n",
    "            )\n",
    "\n",
    "        # Get initializer token ID and initialize placeholder embeddings\n",
    "        token_ids = self.tokenizer.encode(args.initializer_token, add_special_tokens=False)\n",
    "        if len(token_ids) > 1:\n",
    "            raise ValueError(\"The initializer token must be a single token.\")\n",
    "\n",
    "        initializer_token_id = token_ids[0]\n",
    "        self.placeholder_token_ids = self.tokenizer.convert_tokens_to_ids(placeholder_tokens)\n",
    "\n",
    "        # Resize token embeddings\n",
    "        self.text_encoder.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "        # Initialize placeholder token embeddings with initializer token embeddings\n",
    "        token_embeds = self.text_encoder.get_input_embeddings().weight.data\n",
    "        with torch.no_grad():\n",
    "            for token_id in self.placeholder_token_ids:\n",
    "                token_embeds[token_id] = token_embeds[initializer_token_id].clone()\n",
    "\n",
    "        # Freeze models\n",
    "        self.vae.requires_grad_(False)\n",
    "        self.unet.requires_grad_(False)\n",
    "        self.text_encoder.text_model.encoder.requires_grad_(False)\n",
    "        self.text_encoder.text_model.final_layer_norm.requires_grad_(False)\n",
    "        self.text_encoder.text_model.embeddings.position_embedding.requires_grad_(False)\n",
    "\n",
    "        if args.gradient_checkpointing:\n",
    "            self.unet.train()\n",
    "            self.text_encoder.gradient_checkpointing_enable()\n",
    "            self.unet.enable_gradient_checkpointing()\n",
    "\n",
    "        if args.enable_xformers_memory_efficient_attention:\n",
    "            if is_xformers_available():\n",
    "                import xformers\n",
    "                xformers_version = version.parse(xformers.__version__)\n",
    "                if xformers_version == version.parse(\"0.0.16\"):\n",
    "                    logger.warning(\n",
    "                        \"xFormers 0.0.16 cannot be used for training in some GPUs. \"\n",
    "                        \"If you observe problems during training, please update xFormers to at least 0.0.17.\"\n",
    "                    )\n",
    "                self.unet.enable_xformers_memory_efficient_attention()\n",
    "            else:\n",
    "                raise ValueError(\"xformers is not available. Make sure it is installed correctly\")\n",
    "\n",
    "        if args.allow_tf32:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "        if args.scale_lr:\n",
    "            args.learning_rate = (\n",
    "                args.learning_rate * args.gradient_accumulation_steps * \n",
    "                args.train_batch_size * self.accelerator.num_processes\n",
    "            )\n",
    "\n",
    "        # Initialize optimizer\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.text_encoder.get_input_embeddings().parameters(),\n",
    "            lr=args.learning_rate,\n",
    "            betas=(args.adam_beta1, args.adam_beta2),\n",
    "            weight_decay=args.adam_weight_decay,\n",
    "            eps=args.adam_epsilon,\n",
    "        )\n",
    "\n",
    "        # Create dataset\n",
    "        train_dataset = TextualInversionDataset(\n",
    "            data_root=args.train_data_dir,\n",
    "            tokenizer=self.tokenizer,\n",
    "            size=args.resolution,\n",
    "            placeholder_token=(\" \".join(self.tokenizer.convert_ids_to_tokens(self.placeholder_token_ids))),\n",
    "            repeats=args.repeats,\n",
    "            center_crop=args.center_crop,\n",
    "            set=\"train\",\n",
    "        )\n",
    "        \n",
    "        self.train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=args.train_batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=args.dataloader_num_workers\n",
    "        )\n",
    "\n",
    "        # Calculate training steps\n",
    "        overrode_max_train_steps = False\n",
    "        num_update_steps_per_epoch = math.ceil(\n",
    "            len(self.train_dataloader) / args.gradient_accumulation_steps\n",
    "        )\n",
    "        if args.max_train_steps is None:\n",
    "            args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "            overrode_max_train_steps = True\n",
    "\n",
    "        # Create scheduler\n",
    "        self.lr_scheduler = get_scheduler(\n",
    "            args.lr_scheduler,\n",
    "            optimizer=self.optimizer,\n",
    "            num_warmup_steps=args.lr_warmup_steps * self.accelerator.num_processes,\n",
    "            num_training_steps=args.max_train_steps * self.accelerator.num_processes,\n",
    "            num_cycles=args.lr_num_cycles,\n",
    "        )\n",
    "\n",
    "        # Prepare with accelerator\n",
    "        self.text_encoder.train()\n",
    "        (\n",
    "            self.text_encoder, \n",
    "            self.optimizer, \n",
    "            self.train_dataloader, \n",
    "            self.lr_scheduler\n",
    "        ) = self.accelerator.prepare(\n",
    "            self.text_encoder, self.optimizer, self.train_dataloader, self.lr_scheduler\n",
    "        )\n",
    "\n",
    "        # Set weight dtype\n",
    "        weight_dtype = torch.float32\n",
    "        if self.accelerator.mixed_precision == \"fp16\":\n",
    "            weight_dtype = torch.float16\n",
    "        elif self.accelerator.mixed_precision == \"bf16\":\n",
    "            weight_dtype = torch.bfloat16\n",
    "\n",
    "        self.weight_dtype = weight_dtype\n",
    "        self.unet.to(self.accelerator.device, dtype=weight_dtype)\n",
    "        self.vae.to(self.accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "        # Recalculate training steps\n",
    "        num_update_steps_per_epoch = math.ceil(\n",
    "            len(self.train_dataloader) / args.gradient_accumulation_steps\n",
    "        )\n",
    "        if overrode_max_train_steps:\n",
    "            args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "        args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "        # Initialize trackers\n",
    "        if self.accelerator.is_main_process:\n",
    "            self.accelerator.init_trackers(args.wandb_project_name, config=vars(args))\n",
    "\n",
    "        return num_update_steps_per_epoch\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        args = self.config\n",
    "        num_update_steps_per_epoch = self.setup()\n",
    "\n",
    "        total_batch_size = (\n",
    "            args.train_batch_size * \n",
    "            self.accelerator.num_processes * \n",
    "            args.gradient_accumulation_steps\n",
    "        )\n",
    "\n",
    "        logger.info(\"***** Running training *****\")\n",
    "        logger.info(f\"  Num examples = {len(self.train_dataloader.dataset)}\")\n",
    "        logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "        logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
    "        logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "        logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "        logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "\n",
    "        global_step = 0\n",
    "        first_epoch = 0\n",
    "        initial_global_step = 0\n",
    "\n",
    "        # Resume from checkpoint if needed\n",
    "        if args.resume_from_checkpoint:\n",
    "            if args.resume_from_checkpoint != \"latest\":\n",
    "                path = os.path.basename(args.resume_from_checkpoint)\n",
    "            else:\n",
    "                dirs = os.listdir(args.output_dir)\n",
    "                dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
    "                dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
    "                path = dirs[-1] if len(dirs) > 0 else None\n",
    "\n",
    "            if path is None:\n",
    "                self.accelerator.print(\n",
    "                    f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n",
    "                )\n",
    "                args.resume_from_checkpoint = None\n",
    "                initial_global_step = 0\n",
    "            else:\n",
    "                self.accelerator.print(f\"Resuming from checkpoint {path}\")\n",
    "                self.accelerator.load_state(os.path.join(args.output_dir, path))\n",
    "                global_step = int(path.split(\"-\")[1])\n",
    "                initial_global_step = global_step\n",
    "                first_epoch = global_step // num_update_steps_per_epoch\n",
    "\n",
    "        progress_bar = tqdm(\n",
    "            range(0, args.max_train_steps),\n",
    "            initial=initial_global_step,\n",
    "            desc=\"Steps\",\n",
    "            disable=not self.accelerator.is_local_main_process,\n",
    "        )\n",
    "\n",
    "        # Keep original embeddings as reference\n",
    "        orig_embeds_params = (\n",
    "            self.accelerator.unwrap_model(self.text_encoder)\n",
    "            .get_input_embeddings()\n",
    "            .weight.data.clone()\n",
    "        )\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(first_epoch, args.num_train_epochs):\n",
    "            self.text_encoder.train()\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            for step, batch in enumerate(self.train_dataloader):\n",
    "                with self.accelerator.accumulate(self.text_encoder):\n",
    "                    # Convert images to latent space\n",
    "                    latents = self.vae.encode(\n",
    "                        batch[\"pixel_values\"].to(dtype=self.weight_dtype)\n",
    "                    ).latent_dist.sample().detach()\n",
    "                    latents = latents * self.vae.config.scaling_factor\n",
    "\n",
    "                    # Sample noise\n",
    "                    noise = torch.randn_like(latents)\n",
    "                    bsz = latents.shape[0]\n",
    "                    timesteps = torch.randint(\n",
    "                        0, self.noise_scheduler.config.num_train_timesteps, \n",
    "                        (bsz,), device=latents.device\n",
    "                    )\n",
    "                    timesteps = timesteps.long()\n",
    "\n",
    "                    # Add noise to latents\n",
    "                    noisy_latents = self.noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                    # Get text embeddings\n",
    "                    encoder_hidden_states = self.text_encoder(batch[\"input_ids\"])[0].to(\n",
    "                        dtype=self.weight_dtype\n",
    "                    )\n",
    "\n",
    "                    # Predict noise\n",
    "                    model_pred = self.unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "                    # Get target\n",
    "                    if self.noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                        target = noise\n",
    "                    elif self.noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                        target = self.noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            f\"Unknown prediction type {self.noise_scheduler.config.prediction_type}\"\n",
    "                        )\n",
    "\n",
    "                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "\n",
    "                    self.accelerator.backward(loss)\n",
    "                    self.optimizer.step()\n",
    "                    self.lr_scheduler.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                    # Reset embeddings for non-placeholder tokens\n",
    "                    index_no_updates = torch.ones((len(self.tokenizer),), dtype=torch.bool)\n",
    "                    index_no_updates[\n",
    "                        min(self.placeholder_token_ids) : max(self.placeholder_token_ids) + 1\n",
    "                    ] = False\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        self.accelerator.unwrap_model(self.text_encoder).get_input_embeddings().weight[\n",
    "                            index_no_updates\n",
    "                        ] = orig_embeds_params[index_no_updates]\n",
    "\n",
    "                # Accumulate loss for logging\n",
    "                train_loss += loss.detach().item()\n",
    "\n",
    "                # Check if optimization step occurred\n",
    "                if self.accelerator.sync_gradients:\n",
    "                    images = []\n",
    "                    progress_bar.update(1)\n",
    "                    global_step += 1\n",
    "\n",
    "                    # Save embeddings\n",
    "                    if global_step % args.save_steps == 0:\n",
    "                        weight_name = (\n",
    "                            f\"learned_embeds-steps-{global_step}.bin\"\n",
    "                            if args.no_safe_serialization\n",
    "                            else f\"learned_embeds-steps-{global_step}.safetensors\"\n",
    "                        )\n",
    "                        save_path = os.path.join(args.output_dir, weight_name)\n",
    "                        save_progress(\n",
    "                            self.text_encoder,\n",
    "                            self.placeholder_token_ids,\n",
    "                            self.accelerator,\n",
    "                            args,\n",
    "                            save_path,\n",
    "                            safe_serialization=not args.no_safe_serialization,\n",
    "                        )\n",
    "\n",
    "                    # Save checkpoint\n",
    "                    if self.accelerator.is_main_process:\n",
    "                        if global_step % args.checkpointing_steps == 0:\n",
    "                            if args.checkpoints_total_limit is not None:\n",
    "                                checkpoints = os.listdir(args.output_dir)\n",
    "                                checkpoints = [d for d in checkpoints if d.startswith(\"checkpoint\")]\n",
    "                                checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "\n",
    "                                if len(checkpoints) >= args.checkpoints_total_limit:\n",
    "                                    num_to_remove = len(checkpoints) - args.checkpoints_total_limit + 1\n",
    "                                    removing_checkpoints = checkpoints[0:num_to_remove]\n",
    "\n",
    "                                    logger.info(\n",
    "                                        f\"{len(checkpoints)} checkpoints already exist, \"\n",
    "                                        f\"removing {len(removing_checkpoints)} checkpoints\"\n",
    "                                    )\n",
    "\n",
    "                                    for removing_checkpoint in removing_checkpoints:\n",
    "                                        removing_checkpoint = os.path.join(\n",
    "                                            args.output_dir, removing_checkpoint\n",
    "                                        )\n",
    "                                        shutil.rmtree(removing_checkpoint)\n",
    "\n",
    "                            save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n",
    "                            self.accelerator.save_state(save_path)\n",
    "                            logger.info(f\"Saved state to {save_path}\")\n",
    "\n",
    "                        # Run validation\n",
    "                        if args.validation_prompt is not None and global_step % args.validation_steps == 0:\n",
    "                            images = log_validation(\n",
    "                                self.text_encoder,\n",
    "                                self.tokenizer,\n",
    "                                self.unet,\n",
    "                                self.vae,\n",
    "                                args,\n",
    "                                self.accelerator,\n",
    "                                self.weight_dtype,\n",
    "                                epoch,\n",
    "                            )\n",
    "\n",
    "                    # Log metrics\n",
    "                    avg_train_loss = train_loss / args.gradient_accumulation_steps\n",
    "                    logs = {\n",
    "                        \"train_loss\": avg_train_loss,\n",
    "                        \"lr\": self.lr_scheduler.get_last_lr()[0],\n",
    "                        \"epoch\": epoch,\n",
    "                    }\n",
    "                    progress_bar.set_postfix(**logs)\n",
    "                    self.accelerator.log(logs, step=global_step)\n",
    "                    train_loss = 0.0\n",
    "\n",
    "                    if global_step >= args.max_train_steps:\n",
    "                        break\n",
    "\n",
    "            if global_step >= args.max_train_steps:\n",
    "                break\n",
    "\n",
    "        # Save final model\n",
    "        self.accelerator.wait_for_everyone()\n",
    "        if self.accelerator.is_main_process:\n",
    "            if args.push_to_hub and not args.save_as_full_pipeline:\n",
    "                logger.warning(\"Enabling full model saving because --push_to_hub=True was specified.\")\n",
    "                save_full_model = True\n",
    "            else:\n",
    "                save_full_model = args.save_as_full_pipeline\n",
    "                \n",
    "            if save_full_model:\n",
    "                pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "                    args.pretrained_model_name_or_path,\n",
    "                    text_encoder=self.accelerator.unwrap_model(self.text_encoder),\n",
    "                    vae=self.vae,\n",
    "                    unet=self.unet,\n",
    "                    tokenizer=self.tokenizer,\n",
    "                )\n",
    "                pipeline.save_pretrained(args.output_dir)\n",
    "                \n",
    "            # Save final embeddings\n",
    "            weight_name = \"learned_embeds.bin\" if args.no_safe_serialization else \"learned_embeds.safetensors\"\n",
    "            save_path = os.path.join(args.output_dir, weight_name)\n",
    "            save_progress(\n",
    "                self.text_encoder,\n",
    "                self.placeholder_token_ids,\n",
    "                self.accelerator,\n",
    "                args,\n",
    "                save_path,\n",
    "                safe_serialization=not args.no_safe_serialization,\n",
    "            )\n",
    "\n",
    "            if args.push_to_hub:\n",
    "                save_model_card(\n",
    "                    self.repo_id,\n",
    "                    images=images,\n",
    "                    base_model=args.pretrained_model_name_or_path,\n",
    "                    repo_folder=args.output_dir,\n",
    "                )\n",
    "                upload_folder(\n",
    "                    repo_id=self.repo_id,\n",
    "                    folder_path=args.output_dir,\n",
    "                    commit_message=\"End of training\",\n",
    "                    ignore_patterns=[\"step_*\", \"epoch_*\"],\n",
    "                )\n",
    "\n",
    "        self.accelerator.end_training()\n",
    "        logger.info(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef70ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_textual_inversion(\n",
    "    train_data_dir: str,\n",
    "    placeholder_token: str = \"<my-token>\",\n",
    "    initializer_token: str = \"a\",\n",
    "    learnable_property: str = \"object\",\n",
    "    output_dir: str = \"textual-inversion-output\",\n",
    "    pretrained_model_name_or_path: str = \"runwayml/stable-diffusion-v1-5\",\n",
    "    resolution: int = 512,\n",
    "    train_batch_size: int = 4,\n",
    "    learning_rate: float = 5e-4,\n",
    "    max_train_steps: int = 3000,\n",
    "    save_steps: int = 500,\n",
    "    validation_prompt: str = None,\n",
    "    validation_steps: int = 100,\n",
    "    num_validation_images: int = 4,\n",
    "    wandb_project_name: str = \"textual-inversion\",\n",
    "    seed: int = 42,\n",
    "    mixed_precision: str = \"no\",\n",
    "    gradient_accumulation_steps: int = 1,\n",
    "    num_vectors: int = 1,\n",
    "    repeats: int = 100,\n",
    "    center_crop: bool = False,\n",
    "    lr_scheduler: str = \"constant\",\n",
    "    lr_warmup_steps: int = 500,\n",
    "    checkpointing_steps: int = 500,\n",
    "    resume_from_checkpoint: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a textual inversion model.\n",
    "    \n",
    "    Args:\n",
    "        train_data_dir: Path to training data directory containing images and .txt caption files\n",
    "        placeholder_token: Token to learn (e.g., \"<my-cat>\")\n",
    "        initializer_token: Token to initialize embedding (e.g., \"cat\", \"dog\", \"object\", \"style\")\n",
    "        output_dir: Where to save the trained embeddings\n",
    "        pretrained_model_name_or_path: Base Stable Diffusion model\n",
    "        resolution: Training image resolution\n",
    "        train_batch_size: Batch size per device\n",
    "        learning_rate: Learning rate\n",
    "        max_train_steps: Maximum training steps\n",
    "        save_steps: Save embeddings every N steps\n",
    "        validation_prompt: Prompt for validation (should include placeholder_token)\n",
    "        validation_steps: Run validation every N steps\n",
    "        num_validation_images: Number of validation images to generate\n",
    "        wandb_project_name: Weights & Biases project name\n",
    "        seed: Random seed\n",
    "        mixed_precision: \"no\", \"fp16\", or \"bf16\"\n",
    "        gradient_accumulation_steps: Gradient accumulation steps\n",
    "        num_vectors: Number of vectors to learn\n",
    "        repeats: How many times to repeat the training data\n",
    "        center_crop: Whether to center crop images\n",
    "        lr_scheduler: Learning rate scheduler type\n",
    "        lr_warmup_steps: Warmup steps for learning rate\n",
    "        checkpointing_steps: Save checkpoint every N steps\n",
    "        resume_from_checkpoint: Path to checkpoint to resume from\n",
    "    \"\"\"\n",
    "    \n",
    "    config = TrainingConfig(\n",
    "        train_data_dir=train_data_dir,\n",
    "        placeholder_token=placeholder_token,\n",
    "        initializer_token=initializer_token,\n",
    "        learnable_property=learnable_property,\n",
    "        output_dir=output_dir,\n",
    "        pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "        resolution=resolution,\n",
    "        train_batch_size=train_batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        max_train_steps=max_train_steps,\n",
    "        save_steps=save_steps,\n",
    "        validation_prompt=validation_prompt,\n",
    "        validation_steps=validation_steps,\n",
    "        num_validation_images=num_validation_images,\n",
    "        wandb_project_name=wandb_project_name,\n",
    "        seed=seed,\n",
    "        mixed_precision=mixed_precision,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        num_vectors=num_vectors,\n",
    "        repeats=repeats,\n",
    "        center_crop=center_crop,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        lr_warmup_steps=lr_warmup_steps,\n",
    "        checkpointing_steps=checkpointing_steps,\n",
    "        resume_from_checkpoint=resume_from_checkpoint,\n",
    "        report_to=\"wandb\",\n",
    "    )\n",
    "    \n",
    "    trainer = TextualInversionTrainer(config)\n",
    "    trainer.train()\n",
    "    \n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a0570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer = train_textual_inversion(\n",
    "    train_data_dir=\"/kaggle/input/bovagau-poses/images/Bo\",\n",
    "    num_vectors=6,\n",
    "    learnable_property=\"object\",\n",
    "    placeholder_token=\"<Bo>\",\n",
    "    initializer_token=\"anthro\",\n",
    "    resolution=512,\n",
    "    train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_train_steps=5000,\n",
    "    learning_rate=5.0e-04,\n",
    "    lr_scheduler=\"constant\",\n",
    "    output_dir=\"/kaggle/working/bo_text_inver_output\",\n",
    "    mixed_precision=\"fp16\",\n",
    "    checkpointing_steps=500,\n",
    "    validation_prompt=\"a photo of <Bo> in the garden\",\n",
    "    num_validation_images=4,\n",
    "    validation_steps=500,\n",
    "    seed=36,\n",
    "    wandb_project_name=\"textual-inversion\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b9c37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8b2cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Install required packages first:\n",
    "# !pip install diffusers transformers accelerate safetensors wandb\n",
    "\n",
    "# Login to wandb\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "# Prepare your data structure:\n",
    "# data/\n",
    "#   ├── image1.jpg\n",
    "#   ├── image1.txt  (contains caption like \"wearing sunglasses\")\n",
    "#   ├── image2.jpg\n",
    "#   ├── image2.txt  (contains caption like \"sitting on a chair\")\n",
    "#   └── ...\n",
    "\n",
    "# Train the model\n",
    "trainer = train_textual_inversion(\n",
    "    train_data_dir=\"./data\",\n",
    "    placeholder_token=\"<my-cat>\",\n",
    "    initializer_token=\"cat\",\n",
    "    output_dir=\"./output/my-cat\",\n",
    "    validation_prompt=\"A photo of <my-cat> in a garden\",\n",
    "    max_train_steps=3000,\n",
    "    learning_rate=5e-4,\n",
    "    train_batch_size=4,\n",
    "    validation_steps=250,\n",
    "    save_steps=250,\n",
    "    wandb_project_name=\"my-textual-inversion\",\n",
    ")\n",
    "\n",
    "# After training, load and use the embeddings:\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Load the learned embeddings\n",
    "pipe.load_textual_inversion(\"./output/my-cat/learned_embeds.safetensors\")\n",
    "\n",
    "# Generate images\n",
    "image = pipe(\"A photo of <my-cat> playing with a ball\", num_inference_steps=50).images[0]\n",
    "image.save(\"output.png\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genimg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
